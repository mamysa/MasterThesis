\section{Processing Input}

Initially, input to PyPltRedex is read from the file and stored as a string. To be able to apply reduction-relation, the string needs to be analyzed. There are two parts:

\begin{itemize}
\item
Lexical analysis that breaks up the string into individual tokens and decides which kind of token it is.
\item
Parsing - given a set of tokens, produce valid terms.
\end{itemize}


\subsection{Lexical Analysis}

Most commonly tokens are described using regular expressions. Below is description of token kinds supported by PyPltRedex.
\begin{itemize}
\item
	TODO maybe more visual regex display.
\end{itemize}

\subsection{Parsing}

Parsing takes individual lexemes and creates structured data out of them. In this case, terms are produced. Terms have the following grammar:

\begin{lstlisting}
term = term-sequence atom
term-sequence = (term ...) 
atom = integer 
	 | string
	 | (-|+)?[0-9]+\.[0-9]+
	 | #t
	 | #f
	 | identifier
\end{lstlisting}


\subsection{Implementation}

UML diagram for Parser and Tokenizer can be seen below.

TODO diagram


Actual lexer implementation doesn't use all regular expressions as described above but implements their functionality programmatically. The following procedures are defined to detect whitespace, newline and reserved symbols.

\begin{lstlisting}
def is_whitespace(c):
    return c == ' ' or c == '\t' or c == '\n' or c == '\r'

def is_newline(c):
    return c == '\n'

def is_reserved(c): 
    return c in ['(', ')', '[', ']', '{', '}', '\"', '\'', '`', ';', '#', '|', '\\']
\end{lstlisting}


\begin{itemize}
\item
\texttt{string} is the string that requires lexical analysis.

\item
\texttt{start} and \texttt{end} are indices indicating an interval within the \texttt{string}. The substring that ends with index \texttt{start} has already been analyzed. A substring between \texttt{start} and \texttt{end} is a potential token. Any substring after \texttt{end} requires analysis.

\item
\texttt{advance()} method increments \texttt{end} by one.

\item
\texttt{peek()} returns a character at index \texttt{end} of the \texttt{string}.
\texttt{extract\_if\_contains(substring)} extracts substring \texttt{s} beginning at \texttt{start} and ending at \texttt{start+len(substring)} and compares it against provided \texttt{substring}. If both strings are equal, \texttt{start} and \texttt{end} indices are set to \texttt{start+len(substring)} and True is returned. Otherwise, False is returned.

\item
\texttt{extract()} extracts the string between \texttt{start} and \texttt{end}, sets \texttt{start=end} and returns the extracted string.

\item
\texttt{next()} returns the next token in the string. This method implements actual tokenization logic. The algorithm is as follows:
\begin{enumerate}
	\item
	\texttt{peek} the character \texttt{c}.
	
	\item
	If \texttt{is\_whitespace(c)} is True then \texttt{advance} and \texttt{extract}. This will essentially ignores the characters altogether.

	\item
	If \texttt{c} is semicolon, \texttt{advance} and consume any character until newline. \texttt{advance} to consume newline. The result of \texttt{extract} is discarded. If \texttt{end-of-file} is encountered, stop trying to consume characters.

	\item
	if \texttt{c} is one of opening parentheses, return pair \texttt{(LParen, extract())}

	\item
	if \texttt{c} is one of closing parentheses, return pair \texttt{(RParen, extract())}

	\item
	If \texttt{c} is double-quote, \texttt{advance}. Consume characters until matching double-quote is encountered. If during consumption backward slash is encountered, character that immediately follows must also be consumed (i.e. it is escaped character). If \texttt{end-of-string} character is encountered, raise exception as the string doesn't terminate. Return \texttt{(String, extract())}

	\item
	If \texttt{c} is pound symbol \texttt{\#}, call \texttt{extract\_if\_contains} twice with following arguments: \texttt{\#t}, \texttt{\#true} and \texttt{\#f}, \texttt{\#false}. Upon success return \texttt{(Boolean, \#t)} or \texttt{(Boolean, \#f)}. Otherwise, raise Exception.

	\item
	Otherwise, a token must be either an integer, decimal or identifier. To identify these regular expressions are used, but first they have to be extracted. Consume all characters \texttt{c} such that they are not \texttt{is\_whitespace(c)}, \texttt{is\_reserved(c)} or \texttt{end-of-string}. Let \texttt{s=extract()}. 
	\begin{itemize}
		\item
		If \texttt{match(IntegerRegex, s)} return (Integer, s)
		\item
		If \texttt{match(DecimalRegex, s)} return (Float, s)
		\item
		If \texttt{match(IdentRegex  , s)} return (Ident, s)
		\item
		Otherwise, raise Exception - token kind is unknown


	\end{itemize}
	\item
	raise Exception - token kind is unknown.
\end{enumerate}
\end{itemize}

This completes description of the tokenizer. The parser is implemented as a very simple recursive descent parser. Below follows the description of its methods and their functionalities.

\begin{itemize}
\item
\texttt{iseof()} returns true if end of the string has been reached.

\item
\texttt{peek()} returns kind of the next token. 

\item
\texttt{peekv()} returns kind of the next token along with it's value.

\item 
\texttt{expect(expectedkind, tok=None)} throws Exception when \texttt{currenttoken} is not the one expected. In particular,
	\begin{itemize}
		\item
		If \texttt{expectedkind != nexttoken.kind} raise Exception.
		\item
		If \texttt{tok != None}, additionally check if \texttt{tok=nexttoken.value}. Raise Exception if it is not.
		\item
		Otherwise, call \texttt{tokenizer.next()} and assign it to \texttt{nexttoken} and return previous token. 
	\end{itemize}

\item 
	\texttt{parse\_sequence} implements parsing of \texttt{term-sequence} from the grammar above
	\begin{itemize}
		\item
		Let \texttt{seq} be an empty list.

		\item
		The first token is expected to be \texttt{LParen}.

		\item
		While \texttt{peek()} is not \texttt{RParen}, if \texttt{peek()} is \texttt{LParen}, call \texttt{term-sequence} and append its result to \texttt{seq}, otherwise call \texttt{parse\_atom} and append its result to seq.
	
		\item
		\texttt{expect(RParen)} 

		\item
		Return \texttt{Sequence(seq)}
	\end{itemize}

\item
\texttt{parse\_atom} implements parsing of \texttt{atom} from grammar.
	\begin{itemize}
	\item
		If \texttt{peek()} is Integer, return \texttt{Integer(expect(Integer))}
	\item
		If \texttt{peek()} is Float, return \texttt{Float(expect(Decimal))}
	\item

		If \texttt{peek()} is String, return \texttt{String(expect(String))}
	\item
		If \texttt{peek()} is Boolean, return \texttt{Boolean(expect(Boolean))}
	\item

		If \texttt{peek()} is Ident, then \texttt{peekv} to retrieve the value of the token. If value of token is \texttt{hole} return \texttt{Hole()}, otherwise Variable(self.expect(Ident)).
	\end{itemize}
	

\end{itemize}

\subsection{Implementation: Unforseen Complications}
As mentioned mentioned, lexical analysis of integers, floats and identifiers is performed through repeated application of regular expressions which RPython provides support for. The library is perfectly compatible with CPython and regular expression application always succeeds. However, while attempting to feed PyPltRedex-generated code to \texttt{rpython} toolchain results in the following compilation error:

\begin{lstlisting}
[translation:ERROR] AttributeError: 'FrozenDesc' object has no attribute 'pycall'
Processing block:
 block@1149[_choice5_0...] is a <class 'rpython.flowspace.flowcontext.SpamBlock'> 
 in (rpython.rlib.parsing.regexparse:1495)RegexParser._charclass 
 containing the following operations: 
       v1 = simple_call((type set), v0) 
       v2 = simple_call((builtin_function range), (97), (123)) 
       v3 = newlist() 
       v4 = iter(v2) 
       v5 = hint(v3, v2, ({'maxlength': True})) 
\end{lstlisting}

Seems like RPython's \texttt{RegexParser} functionality has a bug. After attempting to investigate, I decided (TODO passive voice) to implement these regular expressions manually instead of relying on the library. This decision was made also because construction of those regular expressions was rather slow.

In addition to character recognizer procedures seen above, the following were added:

\begin{lstlisting}
def is_digit(c):
    return c in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

def is_plusminus(c):
    return c in ['-', '+']

def is_delimeteter(c):
    return is_reserved(c) or c == '\0' or is_whitespace(c)
\end{lstlisting}

State machine for the whole lexical analysis stage can be seen below. Every state in the state machine corresponds to a Python function. First statement in the body of the function consumes a character that satisfies the predicate the incoming edge to the state is labeled with. Accepting states do not consume any input but return desired <tokenkind, tokenvalue> pair.


TODO graphics


\subsection{Future Improvements}
TODO compare performance on big terms. 
float lexing is not faithful .3 and 1. are vaild floats
scientific notation 1e03 is not supported and parsed as identifer.

